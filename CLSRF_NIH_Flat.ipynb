{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIH CXR Database Image Classifier using VGG16 and Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Feature Extractor: NN pretained on ImageNet\n",
    "Output layer(s): SVM - traditional machine learning classifiers\n",
    "Inspired by Sreenivas Bhattiprolu's classifier of similar approach\n",
    "https://github.com/bnsreenu/python_for_microscopists/blob/master/158_classification_CNN_RF.py\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import glob\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "# from keras.models import Model, Sequential\n",
    "# from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "# from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILITY FUCTION FOR TIME STAMPS\n",
    "import calendar\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def getTimeStamp():\n",
    "  # Current GMT time in a tuple format\n",
    "  current_GMT = time.gmtime()\n",
    "\n",
    "  # ts stores timestamp\n",
    "  ts = calendar.timegm(current_GMT)\n",
    "  date_time = datetime.fromtimestamp(ts)\n",
    "  # convert timestamp to string in dd-mm-yyyy HH:MM:SS\n",
    "  return date_time.strftime(\"%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: []  \n",
      "CPU: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]  \n",
      "Python 3.10.5 | packaged by conda-forge | (main, Jun 14 2022, 07:03:09) [Clang 13.0.1 ]\n",
      "Tensorflow2.9.1\n"
     ]
    }
   ],
   "source": [
    "# SHOW US WHAT YAH GOT\n",
    "import sys\n",
    "print(f\"GPU: {tf.config.list_physical_devices('GPU')}  \")\n",
    "print(f\"CPU: {tf.config.list_physical_devices('CPU')}  \")\n",
    "print(f\"Python {sys.version}\\nTensorflow{tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/white/Documents/AlexProjects/DICOM_OCC_MED/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORKING_DIR_TS = os.getcwd()\n",
    "WORKING_DIR_TS += '/'\n",
    "if WORKING_DIR_TS == '/content/': #will be true on google colab\n",
    "  WORKING_DIR_TS = '/content/drive/MyDrive/' #this is where files actually reside on google colab (that is on a mounted google drive)\n",
    "WORKING_DIR_TS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of full dataset: (112120, 11)\n",
      "Shape of PA only dataset: (67310, 11)\n",
      "NUMBER OF PA STUDIES WITH FINDING\n",
      "No Finding    39302\n",
      "Fibrosis        648\n",
      "Emphysema       525\n",
      "Name: Finding Labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# define constansts\n",
    "SIZE = 512  #Resize images, note original input shape for VGG is (224,244,3)\n",
    "MAX_NUMBER_OF_EACH_FINDING = 525 #Total number of image for both testing and training\n",
    "df = pd.read_csv(WORKING_DIR_TS+'Data_Entry_2017_v2020.csv')\n",
    "df = df[df['Patient Age']<100] #removing datapoints which having age greater than 100\n",
    "print(f\"Shape of full dataset: {df.shape}\")\n",
    "PA_df = df[df['View Position'] == 'PA'] #Fileter just for PA view images (exclude AP)\n",
    "print(f\"Shape of PA only dataset: {PA_df.shape}\")\n",
    "fullMatrchString = \"No Finding|Fibrosis|Emphysema\"\n",
    "filtered_df = PA_df[PA_df['Finding Labels'].str.fullmatch(fullMatrchString)]; # Filter for a selection of parenchymal diseases.  Ie Cardiomegally, pleural thickening, pneumothorax excluded.  Note consolidation & pneumonia also excluded (consolidation, infiltration, PNA are not all exclusive descriptive terms; PNAs can be very different in appearance - round PNA vs lobal PNA vs viral PNA); Did not include edema because there are only 51 studies from 10 unique patients\n",
    "# filtered_df = filtered_df.drop_duplicates(subset=['Patient ID'],ignore_index=True)  #UNIQUE PATIENTS ONLY\n",
    "print(f\"NUMBER OF PA STUDIES WITH FINDING\\n{ filtered_df['Finding Labels'].value_counts()}\") \n",
    "total_df=filtered_df.groupby('Finding Labels').head(MAX_NUMBER_OF_EACH_FINDING) ### cut down the count of each type to a max number, some gropus may have less than the max (PA edema is a small group)\n",
    "# print(pd.crosstab(df['View Position'],df['Finding Labels']).head(10));\n",
    "# test_df= filtered_df.groupby('Finding Labels').head(NUMBER_OF_EACH_FINDING_RESERVED_FOR_TESTING).reset_index(drop=True)\n",
    "# train_df = total_df[~total_df['Image Index'].isin(test_df['Image Index'].to_list())]\n",
    "# print(test_df)\n",
    "# # SANITY CHECK PRINTS\n",
    "# print(total_df.head()[\"Image Index\"])\n",
    "# print(test_df.head()[\"Image Index\"])\n",
    "# print(train_df.head()[\"Image Index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to get the the images and their labels from a FLAT directiory (NO subfolders)\n",
    "def getImagesAndLabels(patients_df):\n",
    "  labels = []\n",
    "  images = []\n",
    "  image_paths = patients_df['Image Index'].to_numpy()\n",
    "  labels = patients_df['Finding Labels'].to_numpy()\n",
    "  for img_path in image_paths:\n",
    "    img_path = WORKING_DIR_TS+\"dicom/NIH_images/\"+img_path\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "    img = cv2.resize(img, (SIZE, SIZE))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    images.append(img)\n",
    "\n",
    "  return np.array(images), np.array(labels)\n",
    "  #np.count_nonzero(Label_Array == 'No Finding')\n",
    "  #Label_Array.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images, labels = getImagesAndLabels(total_df)\n",
    "# train_images, train_labels = getImagesAndLabels(train_df)\n",
    "# test_images, test_labels = getImagesAndLabels(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fibrosis', 'Emphysema', 'Emphysema', 'Emphysema', 'No Finding'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_images, test_images, train_labels,  test_labels = train_test_split(images, labels, random_state=0, test_size = 0.2, stratify=labels)\n",
    "train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the split between train and test sets.  By setting Straify = Labels it should keep the values proportional\n",
    "unique_labels, train_labels_counts = np.unique(train_labels, return_counts=True)\n",
    "#Show the split between train and test sets.  By setting Straify = Labels it should keep the values proportional\n",
    "unique_labels, train_labels_counts ,np.unique(test_labels, return_counts=True), unique_labels.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels, ie from 'No Finding', 'Fibrosis', ... ====> 0, 1, ...\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(test_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "label_encoder.fit(train_labels)\n",
    "train_labels_encoded = label_encoder.transform(train_labels)\n",
    "label_encoder.classes_,train_labels[:5],train_labels_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename to standard naming conventions\n",
    "x_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RESCALE AND FORMAT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize pixel values to between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode y values for neural network. \n",
    "# Not currently used.  Will keep in case want to used in futures.\n",
    "y_train_one_hot = keras.utils.to_categorical(y_train)\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test)\n",
    "y_train_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "x_train[0,0,0] #should be greyscale pixel (R=G=B) in range O to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRANSFER LEARNING  LOAD PRETRAINED KERAS APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model without output classifier/fully connected layers, set input shape to our custom SIZE \n",
    "#This will act as a \"feature detector\"\n",
    "IMAGE_SHAPE = (SIZE, SIZE)\n",
    "print(IMAGE_SHAPE)\n",
    "base_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False,  input_shape=(SIZE,SIZE,3), pooling='max')\n",
    "base_model.trainable = False\n",
    "inputs = keras.Input(shape=(SIZE, SIZE, 3))\n",
    "# We make sure that the base_model is running in inference mode here\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "intermediate_outputs = base_model(inputs, training=False)\n",
    "#base_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CREATE OUTPUT LAYERS AND ADD TO THE BASE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE A SVM (SUPPORT VECTOR MACHINE) FOR CLASSIFYING\n",
    "### SVM example: https://github.com/krishnaik06/Complete-Deep-Learning/blob/master/Image%20Classification%20Using%20SVM.ipynb\n",
    "# # #FLATTEN - MAY NOT BE NEEDED WITH SOME BASE MODELS DEPENDING OUT OUTPUT SHAPE, COULD DO GLOBALMAXPOOLING INSTEAD\n",
    "# # x = keras.layers.Flatten()(x)\n",
    "# # # Make Fully Connection Layer\n",
    "# x = keras.layers.Dense(units=16, activation='relu')(intermediate_outputs)\n",
    "# # USE LINEAR CLASSIFICATION FOR A BINARY CLASSIFIER\n",
    "# outputs = keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='linear')(x) \n",
    "# # # USE LINEAR CLASSIFICATION FOR A BINARY CLASSIFIER\n",
    "# # outputs = keras.layers.Dense(XXXXCATEGORYNUMBER, kernel_regularizer=tf.keras.regularizers.l2(0.01),activation='softmax')(x) \n",
    "\n",
    "\n",
    "### Create a simple final dense NN layer\n",
    "\n",
    "outputs = tf.keras.layers.Dense( unique_labels.size, activation = tf.keras.activations.softmax)(intermediate_outputs)\n",
    "print(\"Model outputs categories/labels: \", unique_labels.size)\n",
    "\n",
    "### CREATE OUR FINAL MODEL\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# #Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights\n",
    "# for layer in model.layers:\n",
    "# \tlayer.trainable = False\n",
    "model.summary()  #Trainable parameters will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "model.compile(\n",
    "  optimizer = 'adam', \n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),#  logits=TrUe /// loss = 'hinge', \n",
    "  metrics = ['accuracy']) #'sparse_categorical_crossentropy'\n",
    "# generate path wehere you want to save your checkpoints\n",
    "checkpoint_path = f\"checkpoints/{getTimeStamp()}_weights.h5\"\n",
    "# define a callback that will save the model weights after every epoch \n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_path,\n",
    "  #save_weights_only=True,\n",
    "  monitor='val_accuracy',\n",
    "  mode='max',\n",
    "  verbose=1,\n",
    "  save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the CNN model\n",
    "EPOCHS=20\n",
    "BATCH_SIZE=16\n",
    "history = model.fit(\n",
    "  x_train, y_train, \n",
    "  epochs=EPOCHS, \n",
    "  batch_size=BATCH_SIZE, \n",
    "  validation_data = (x_test, y_test), \n",
    "  callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model weights from file\n",
    "# if(False): ##I don't want this to run noramlly\n",
    "new_model = keras.models.load_model(checkpoint_path)\n",
    "  # assert_allclose(model.predict(x_train), new_model.predict(x_train), 1e-5)\n",
    "# continue fitting the model\n",
    "# generate a new path wehere you want to save your checkpoints\n",
    "checkpoint_path = f\"checkpoints/{getTimeStamp()}_weights.h5\"\n",
    "# define a callback wutg the new path name\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath=checkpoint_path,\n",
    "  save_weights_only=True,\n",
    "  monitor='val_accuracy',\n",
    "  mode='max',\n",
    "  verbose=1,\n",
    "  save_best_only=True)\n",
    "##RESTART TRAINING\n",
    "history = new_model.fit( \n",
    "  x_train, y_train, \n",
    "  epochs=EPOCHS, \n",
    "  batch_size=BATCH_SIZE, \n",
    "  validation_data = (x_test, y_test), \n",
    "  callbacks=[model_checkpoint_callback])\n",
    "model= keras.models.load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize training\n",
    "fig, (ax1,bx1), = plt.subplots(1, 2, sharey=True, figsize=(15, 5))\n",
    "l1, = ax1.plot(history.history['loss'], color='blue') \n",
    "ax2 = ax1.twinx()\n",
    "l2, = ax2.plot(history.history['accuracy'], color='red')\n",
    "ax1.set_ylabel('loss' , color='blue')\n",
    "ax2.set_ylabel('accuracy', color='red' )\n",
    "plt.legend([l1, l2], [\"loss\",\"accuracy\" ], loc=\"upper center\")\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Summarize Training')\n",
    "plt.figtext(0.5, 0.01, 'footnote text\\n more text \\n asdf', horizontalalignment='center', fontsize=8)\n",
    "# plt.show()\n",
    "\n",
    "# summarize validation\n",
    "# fig, ax1 = plt.subplots()\n",
    "l1, = bx1.plot(history.history['val_loss'], color='blue') \n",
    "bx2 = bx1.twinx()\n",
    "l2, = bx2.plot(history.history['val_accuracy'], color='red')\n",
    "bx1.set_ylabel('val_loss' , color='blue')\n",
    "bx2.set_ylabel('val_accuracy', color='red' )\n",
    "plt.legend([l1, l2], [\"val_loss\",\"val_accuracy\" ], loc=\"upper center\")\n",
    "plt.xlabel('epochs')\n",
    "plt.title('Summarize Validation')\n",
    "plt.show()\n",
    "\n",
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper right')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXTRACT FEATURES USING THE PRETRAINED CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Send test data through the model to get the output values\n",
    "X_test_feature = model.predict(x_test)\n",
    "\n",
    "# X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feature[:9],X_test_feature.shape\n",
    "flattened = X_test_feature.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKE A RANDOM FOREST LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RF_model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "RF_model.fit(X_for_RF, y_train) #For sklearn no one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now predict using the trained RF model. \n",
    "prediction_RF = RF_model.predict(X_test_features)\n",
    "#Inverse le transform to get original label back. \n",
    "prediction_RF = label_encoder.inverse_transform(prediction_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print overall accuracy\n",
    "accuracy = metrics.accuracy_score(test_labels, prediction_RF)\n",
    "print (\"Accuracy = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "#Confusion Matrix - verify accuracy of each class\n",
    "cm = metrics.confusion_matrix(test_labels, prediction_RF)\n",
    "#print(cm)\n",
    "heatmap =sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save confusion matrix\n",
    "heatmap.get_figure().savefig(f\"confusion_matrices/{getTimeStamp()}_VGG16-RF_cats{cm.shape[0]}-{MAX_NUMBER_OF_EACH_FINDING}-{NUMBER_OF_EACH_FINDING_RESERVED_FOR_TESTING}_size{SIZE}_acc{round(accuracy,2)}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check results on a few select images\n",
    "n=np.random.randint(0, x_test.shape[0])\n",
    "img = x_test[n]\n",
    "plt.imshow(img)\n",
    "input_img = np.expand_dims(img, axis=0) #Expand dims so the input is (num images, x, y, c)\n",
    "input_img_feature=model.predict(input_img)\n",
    "input_img_features=input_img_feature.reshape(input_img_feature.shape[0], -1)\n",
    "prediction_RF = RF_model.predict(input_img_features)[0] \n",
    "prediction_RF = label_encoder.inverse_transform([prediction_RF])  #Reverse the label encoder to original name\n",
    "print(\"The prediction for this image is: \", prediction_RF)\n",
    "print(\"The actual label for this image is: \", test_labels[n])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('Transfer_Learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "342dfa87ede328b7be74dd9e2a5befe9fb3b3b15191910f7b9d300f82b7ef8a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
